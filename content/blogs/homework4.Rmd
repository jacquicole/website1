---
title: "Machine Learning Exercise"
date: "2023-06-21"
description: A classification problem
draft: no
image: "machine-learning-image.jpg"
keywords: ''
slug: homework4
categories:
- ''
- ''
---
#---
#title: 'Homework 4: Machine Learning'
#author: "Jacqui Cole"
#date: "`r Sys.Date()`"
#output:
#  word_document:
#    toc: yes
#  pdf_document:
#    toc: yes
#  html_document:
#    theme: flatly
#    highlight: zenburn
#    number_sections: yes
#    toc: yes
#    toc_float: yes
#    code_folding: show
#---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false
options(scipen = 999) #disable scientific notation
library(tidyverse)
library(tidymodels)
library(GGally)
library(sf)
library(leaflet)
library(janitor)
library(rpart.plot)
library(here)
library(scales)
library(vip)
```

# The Bechdel Test

<https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/>

The [Bechdel test](https://bechdeltest.com) is a way to assess how women are depicted in Hollywood movies. In order for a movie to pass the test:

1.  It has to have at least two [named] women in it
2.  Who talk to each other
3.  About something besides a man

There is a nice article and analysis you can find here <https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/> We have a sample of 1394 movies and we want to fit a model to predict whether a film passes the test or not.

```{r read_data}

bechdel <- read_csv(here::here("data", "bechdel.csv")) %>% 
  mutate(test = factor(test)) 
glimpse(bechdel)

```

How many films fail/pass the test, both as a number and as a %?

```{r}
bechdel %>% #read in dataset of movies
  count(test) %>% #calculate proportion of movies that fail/pass the test
  mutate(percentfail = 100*n/sum(n)) #calculate corresponding percentage

#The answers are found in the table below:
```

## Movie scores

```{r}
ggplot(data = bechdel, aes(
  x = metascore,
  y = imdb_rating,
  colour = test
)) +
  geom_point(alpha = .3, size = 3) +
  scale_colour_manual(values = c("tomato", "olivedrab")) +
  labs(
    x = "Metacritic score",
    y = "IMDB rating",
    colour = "Bechdel test"
  ) +
 theme_light()
```

# Split the data

```{r}
# **Split the data**

set.seed(123)

data_split <- initial_split(bechdel, # updated data
                           prop = 0.8, 
                           strata = test)

bechdel_train <- training(data_split) 
bechdel_test <- testing(data_split)
```

Check the counts and % (proportions) of the `test` variable in each set.

```{r}
bechdel_train %>% #read in training dataset of movies
  count(test) %>% #calculate proportion of movies that fail/pass the test
  mutate(percentfail = 100*n/sum(n)) #calculate corresponding percentage

#The training data set has the following counts and proportion of Fail/Pass:
```

```{r}
bechdel_test %>% #read in test dataset of movies
  count(test) %>% #calculate proportion of movies that fail/pass the test
  mutate(percentfail = 100*n/sum(n)) #calculate corresponding percentage

#The training data set has the following counts and proportion of Fail/Pass:
```

## Feature exploration

## Any outliers?

```{r}

bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore) %>% 

    pivot_longer(cols = 2:6,
               names_to = "feature",
               values_to = "value") %>% 
  ggplot()+
  aes(x=test, y = value, fill = test)+
  coord_flip()+
  geom_boxplot()+
  facet_wrap(~feature, scales = "free")+
  theme_bw()+
  theme(legend.position = "none")+
  labs(x=NULL,y = NULL) 
  

```

```{r}

#There are no massive outliers, although the maximum value of budget 2013 seems to be disproportionately larger than others. So I find the maximum value of this outlier:
 bechdel %>%
  summarise(maxi = max(budget_2013)) 
 
```

```{r}
#The identity of this outlier is deduced by:

 bechdel %>%
 filter (budget_2013 > 46)    
 
#The outlier is Avatar. I note that its domestic gross (domgross_2013 is also the maximum value of that field (302), but it is not the max value of the intgross (although it is high at 82.5)). Overall, when considered over the full range of the variables plotted in the above boxplots, it is not an outlier in all regards. So, I keep it in the dataset in going forward. Though I just note its disproportionately large value for a few of its variables.
```

## Scatterplot - Correlation Matrix

Write a paragraph discussing the output of the following

```{r, warning=FALSE, message=FALSE}
bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore)%>% 
  ggpairs(aes(colour=test), alpha=0.2)+
  theme_bw()

#The output of the following correlation matrix tests reveal that:
#(a) the variables have quite different distributions 
#- metascore is somewhere between a trianglular and normal distribution;
#- imbb_rating appears to be a fairly normal distribution albeit with a small left skew;
#-the budget_2013, domgross_2013 and intgross_2013 variables show a near Voigt function that is cut off on the left and which is highly right skewed, as most data are of low values.
#(b) The test data (Fail and Pass) seem to be fairly even in number and the distribution of Fail and Pass data, as judged by the similar profiles showing in the plots that comprise the left hand column below the test histogram. This evenness is important because it shows that the test data are balanced when it comes to a machine-learning application. A corrolaary to this is t also shows that a random partitioning of the data into a training and testing dataset should result in a balanced set of split datasets.
#(c) The correlation values for the total datasets (in black font) show that there are significant levels of correlation between the following variables:
#
#  52% correlation between domgross_2013 and budget_2013
#  62% correlation between intgross_2013 and budget_2013
#  94% correlation between intgross_2013 and domgross_2013
#  74% correlation between metascore and imdb_rating
#
# This makes intuitive sense because one would hope that the larger the budget for a film, the more that the film will profit in both domestic and international gross. As a corrolary, one would expect a correlation between domestic and international gross, assuming that most films are international (as hollywood films tend to be).
#
# Meanwhile, one would intuitively expect there to be a correlation in ratings: the imdb_rating and the metascore in this case.
#
# Note that there are no correlations between the scoring-based variables and the budget variable, which means that the audience appreciation level is independent of the budget spent on the film!
#
# These ratings are slightly correlated to the gross profits of the films which suggests that customers choose to see big budget films a bit more than others, perhaps expecting a good experience. However, the lack of correlation between ratings and budget suggest that the audience is occasionally let down by a big budget film.
#
# There is little variation between the correlations of the total data versus those that Pass or Fail these 3 criteria. However, there are a few observable differences: e.g. domgross_2013 and budget_2013 are slightly less correlated if the criteria are not met (Fail) and slightly more correlated if the criteria are met. 
# There is also a slightly lower correlation between film ratings/scores and gross profits of films if the criteria are met (Pass) than if they are not met (Fail).
# 
#This all means that there appears to be a modest but statistically significant relationship in movies and these criteria being met.

```

## Categorical variables

Write a paragraph discussing the output of the following

```{r}
bechdel %>% 
  group_by(genre, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
  
 
bechdel %>% 
  group_by(rated, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))

#
# This output shows that a greater proportion of movies meet the #aforementioned
# three criteria whatever the movie rating. However, the proportion differs depending upon the rating: whereby there are two ratings with notable differences between Pass/Fail:

#a G rating shows a significantly higher proportion (62:38) of movies that fail these criteria; perhaps this is because many animations would fall into the G rating category which don't so often have gender specific characters portrayed. 
#
#a NC-17 rating whereby such movies are categorised as showing highly sexualised content. This has the highest (by some margin) disparity between proportions (83:17) of movies that fail the subject criteria. That said, this category contains the least number of movies (6 in total) so one could argue that the sample is too small; although 5:1 is still compelling given the distinctive categorisation of the movie rating and how this aligns with the female focused nature of the criteria.
#
#The other ratings have only a modest difference between Fail/Pass in the criteria with a representative (large) number of movies in the sample (the largest disparity amongst all 3 rating types (R, PG-13, PG)) being 56:44
```

# Train first models. `test ~ metascore + imdb_rating`

```{r}
lr_mod <- logistic_reg() %>% 
  set_engine(engine = "glm") %>% 
  set_mode("classification")

lr_mod


tree_mod <- decision_tree() %>% 
  set_engine(engine = "C5.0") %>% 
  set_mode("classification")

tree_mod 
```

```{r}


lr_fit <- lr_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )

tree_fit <- tree_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )
```

## Logistic regression

```{r}
lr_fit %>%
  broom::tidy()

lr_preds <- lr_fit %>%
  augment(new_data = bechdel_train) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0))

```

### Confusion matrix

```{r}
lr_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")


```

## Decision Tree

```{r}
tree_preds <- tree_fit %>%
  augment(new_data = bechdel) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0)) 


```

```{r}
tree_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

## Draw the decision tree

```{r}
draw_tree <- 
    rpart::rpart(
        test ~ metascore + imdb_rating,
        data = bechdel_train, # uses data that contains both birth weight and `low`
        control = rpart::rpart.control(maxdepth = 5, cp = 0, minsplit = 10)
    ) %>% 
    partykit::as.party()
plot(draw_tree)

```

# Cross Validation

Run the code below. What does it return?

```{r}
set.seed(123)
bechdel_folds <- vfold_cv(data = bechdel_train, 
                          v = 10, 
                          strata = test)
bechdel_folds

# This code returns the labelling of a 10-fold cross-validation.
```

## `fit_resamples()`

Trains and tests a resampled model.

```{r}
lr_fit <- lr_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )


tree_fit <- tree_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )
```

## `collect_metrics()`

Unnest the metrics column from a tidymodels `fit_resamples()`

```{r}

collect_metrics(lr_fit)
collect_metrics(tree_fit)


```

```{r}
tree_preds <- tree_mod %>% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds,
    control = control_resamples(save_pred = TRUE) #<<
  )

# What does the data for ROC look like?
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail)  

# Draw the ROC
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail) %>% 
  autoplot()

# The data for ROC show that the model is not very accurate at all, because the True Positive and True Negatives (matrix diagonal) in the confusion matrix are not very distinguished from the False Negatives and False Positives (off diagonals). Indeed, the results are barely above the dotted line (x = y) which would be the case for data from random guessing.
```

# Build a better training set with `recipes`

## Preprocessing options

-   Encode categorical predictors
-   Center and scale variables
-   Handle class imbalance
-   Impute missing data
-   Perform dimensionality reduction
-   ... ...

## To build a recipe

1.  Start the `recipe()`
2.  Define the variables involved
3.  Describe **prep**rocessing [step-by-step]

## Collapse Some Categorical Levels

Do we have any `genre` with few observations? Assign genres that have less than 3% to a new category 'Other'

```{r}
bechdel %>% #reading in the full dataset
  count(genre) %>% # looking at the distribution of genres
  mutate(m = 100*n/sum(n)) %>% # calculating the genre proportion
  filter (m < 3) # identifying the genres that appear less than 3 percent.
#
# The resulting table below identifies the six genre categories that make up less than 3 percent of the dataset.
```

```{r}
#| echo = FALSE
bechdel %>% 
  count(genre) %>% 
  mutate(genre = fct_reorder(genre, n)) %>% 
  ggplot(aes(x = genre, 
             y = n)) +
  geom_col(alpha = .8) +
  coord_flip() +
  labs(x = NULL) +
  geom_hline(yintercept = (nrow(bechdel_train)*.03), lty = 3)+
  theme_light()
```

```{r}
movie_rec <-
  recipe(test ~ .,
         data = bechdel_train) %>%
  
  # Genres with less than 5% will be in a catewgory 'Other'
    step_other(genre, threshold = .03) 
```

## Before recipe

```{r}
#| echo = FALSE
bechdel_train %>% 
  count(genre, sort = TRUE)
```

## After recipe

```{r}
movie_rec %>% 
  prep() %>% 
  bake(new_data = bechdel_train) %>% 
  count(genre, sort = TRUE)
```

## `step_dummy()`

Converts nominal data into numeric dummy variables

```{r}
#| results = "hide"
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_dummy(all_nominal_predictors()) 

movie_rec 
```

## Let's think about the modelling

What if there were no films with `rated` NC-17 in the training data?

```{r}
#Then, the model will not consider this NC-17 rating.
```

-   Will the model have a coefficient for `rated` NC-17?

```{r}
# The model will not have any coefficient for rated NC-17.
```

-   What will happen if the test data includes a film with `rated` NC-17?

```{r}
# The model will presumably ignore the rated NC-17 class of data.
```

## `step_novel()`

Adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set.

```{r}

movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal_predictors) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal_predictors()) 

```

## `step_zv()`

Intelligently handles zero variance variables (variables that contain only a single value)

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes()) 
  
```

## `step_normalize()`

Centers then scales numeric variable (mean = 0, sd = 1)

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) 

```

## `step_corr()`

Removes highly correlated variables

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric())  
 # step_corr(all_predictors(), threshold = 0.75, method = "spearman") 



movie_rec
```

# Define different models to fit

```{r}
## Model Building

# 1. Pick a `model type`
# 2. set the `engine`
# 3. Set the `mode`: regression or classification

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 
```

# Bundle recipe and model with `workflows`

```{r}
log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(movie_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow


## A few more workflows

tree_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(tree_spec) 

rf_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(rf_spec) 

xgb_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(xgb_spec)

knn_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(knn_spec)

```

HEADS UP

1.  How many models have you specified?

```{r}
#Five models have been specified in the above code run and these take the form of various types of classification: logistic regression (classification), a decision tree (tree), and various tree options: a random forest (rf), a booststrapping option: gradient boosting (xgb) and k-nearest neighbours (knn).
```

1.  What's the difference between a model specification and a workflow?

```{r}
#A model specification is essentially a function that one can use to apply unseen data that can fit well to the model if the data used to generate the function are similar to the unseen data. In this case, each model is a classifier which means that it partitions data into certain categories according to its relative fit to the underpinning model that was defined by the training data.

#In contrast, a workflow is a sequence of steps or operations that lead to a result. In the workflows above, a model is contained within each of them.
```

1.  Do you need to add a formula (e.g., `test ~ .`) if you have a recipe?

```{r}
#No, because the recipe contains a script to clean the data and prepare it for being apply to the data model via the workflow. The model contains formula(e) but that is called within the workflow after the recipe has been executed.
```

# Model Comparison

You now have all your models. Adapt the code from slides `code-from-slides-CA-housing.R`, line 400 onwards to assess which model gives you the best classification.

```{r}

## Evaluate Models

## Logistic regression results{.smaller}

log_res <- log_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy,
      kap, roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)) 

# Show average performance over all folds (note that we use log_res):
log_res %>%  collect_metrics(summarize = TRUE)

# Show performance for every single fold:
log_res %>%  collect_metrics(summarize = FALSE)



## `collect_predictions()` and get confusion matrix{.smaller}

log_pred <- log_res %>% collect_predictions()

log_pred %>%  conf_mat(test, .pred_class) 

log_pred %>% 
  conf_mat(test, .pred_class) %>% 
  autoplot(type = "mosaic") +
  geom_label(aes(
      x = (xmax + xmin) / 2, 
      y = (ymax + ymin) / 2, 
      label = c("TP", "FN", "FP", "TN")))


log_pred %>% 
  conf_mat(test, .pred_class) %>% 
  autoplot(type = "heatmap")


## ROC Curve

log_pred %>% 
  group_by(id) %>% # id contains our folds
  roc_curve(test, .pred_Pass) %>% 
  autoplot()


## Decision Tree results

tree_res <-
  tree_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

tree_res %>%  collect_metrics(summarize = TRUE)


## Random Forest

rf_res <-
  rf_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

rf_res %>%  collect_metrics(summarize = TRUE)

## Boosted tree - XGBoost

xgb_res <- 
  xgb_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

xgb_res %>% collect_metrics(summarize = TRUE)

## K-nearest neighbour

knn_res <- 
  knn_wflow %>% 
  fit_resamples(
    resamples = bechdel_folds, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

knn_res %>% collect_metrics(summarize = TRUE)


## Model Comparison

log_metrics <- 
  log_res %>% 
  collect_metrics(summarise = TRUE) %>%
  # add the name of the model to every row
  mutate(model = "Logistic Regression") 

tree_metrics <- 
  tree_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Decision Tree")

rf_metrics <- 
  rf_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Random Forest")

xgb_metrics <- 
  xgb_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost")

knn_metrics <- 
  knn_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Knn")

# create dataframe with all models
model_compare <- bind_rows(log_metrics,
                           tree_metrics,
                           rf_metrics,
                           xgb_metrics,
                           knn_metrics) 

#Pivot wider to create barplot
  model_comp <- model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean are under the curve (ROC-AUC) for every model
model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>% # order results
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 2), 
         y = mean_roc_auc + 0.08),
     vjust = 1
  )+
  theme_light()+
  theme(legend.position = "none")+
  labs(y = NULL)

## `last_fit()` on test set

# - `last_fit()`  fits a model to the whole training data and evaluates it on the test set. 
# - provide the workflow object of the best model as well as the data split object (not the training data). 
 
last_fit_xgb <- last_fit(xgb_wflow, 
                        split = data_split,
                        metrics = metric_set(
                          accuracy, f_meas, kap, precision,
                          recall, roc_auc, sens, spec))

last_fit_xgb %>% collect_metrics(summarize = TRUE)

#Compare to training
xgb_res %>% collect_metrics(summarize = TRUE)


## Variable importance using `{vip}` package

library(vip)

last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10) +
  theme_light()


## Final Confusion Matrix

last_fit_xgb %>%
  collect_predictions() %>% 
  conf_mat(test, .pred_class) %>% 
  autoplot(type = "heatmap")


## Final ROC curve
last_fit_xgb %>% 
  collect_predictions() %>% 
  roc_curve(test, .pred_Fail) %>% 
  autoplot()
```

```{r}
#Assessing which model affords the best classification from these results:
#
# First, considering the results from each model in turn: 
#
# The logistic regression performs poorly, as judged by the fact that: (i) the algorithm to firm up the model did not converge (as highlighted above in the warning statements against the glm.fit); (ii) the statistical metrics that showcase overall average performance over all folds are all around (and all but one are less than) 50% (or zero in the case of the 'kap' metric). This means that the model does not perform better than random guessing, i.e. the model carries no relevant information or insight and is useless. (iii) analogous performance metrics over each fold are overall just as bad as described in (ii). (iv) There is little discrimination between any elements in the confusion matrix (all range from 243-328) with possibly just a hint of a possible correlation of note for the True negative (Fail-Truth and Pass-Prediction) but the effect is slight. (v) The ROC curves from analysis of each fold lie fairly evenly above and below the y=x line, revealing that the results overall of the model are no better than a random guess. No particular fold is far better than any other; fold 7 is perhaps a bit better than the others but its results are not great in themselves either. 
#
# The decision tree model fares a bit better, with evaluation metrics mostly in the high 50s or early 60s (except for kep which is above zero). This suggests that the model does fit some of the data with a priori information to the extent that it is better than random guessing. 
#
# The random forest results compare more favourably in terms of these same evaluation metrics, with most metrics ranging from 0.64-0.78. This could be considered to be quite strong as a model; indeed, models that are too high (a perfect one would be 100%) can suffer from being too highly correlated owing to effects such as overfitting. 
#
# The bootstrapping decision-tree option, gradient boosting, fares similarly to that of the random forest model, with overall results suggesting that this model is just slightly interior in comparison.
#
# the k-nearest neighbour classification model does not perform very well overall, although it does have high sensitivity and its f-score is over 70%; this is correlated to its high recall of 90% and the lower precision of 55% which brings down the overall f-score.
#
# A graphical summary of the mean ROC metric for each model is given above. This offers a quick realisation of the salient conclusions of the model evaluation interpretations that have been described in more detail above. In short, the logistic regression model is so poor that it is statistically worse than the case of random guessing! The k-nearest neighbour model is a pretty simple classification model and thus also performs fairly poorer (but better than logistic regression!). The next more complicated decision-tree model fares better while the random forest and gradient boosting (bootstrapping) models offer the best fits to the training data. The random forest model will train many trees independently while a gradient boosting model will train many trees subsequently (correcting the errors in a given tree from that of its forerunner tree). Thus, the gradient boosting model has bootstrapping functionality, and its results are easier to interpret than that of a random forest model because gradient boosting models are afforded from one final tree while the result of a random forest model are the ensemble of many trees that have been modelled in parallel. Given these fundemental model considerations, and the similar ROC of these two model options, the gradient boosting model was deemed to be the best model for being taken forward in classifying the test (unseen) movie data. 
#The results of applying these test data to the gradient boosting model are good, albeit less good (values of most metrics are in mid-60s) than the results from the training data (where metric values extended to the 70s). This comparision stands to reason since unseen data will naturally fit less well than the training data which were used to create (i.e. were tailored to the fit of) the model. Besides, the results of testing and training data still fare well in having fairly small overall differences.
#
# The results of the gradient boosting model were then interpreted in terms of a feature selection i.e. an assessment of the level of importance of each variable that is in the movie dataset and was applied to train the model. A graphical display of this extent of importance per movie variable is shown in the histogram above. It shows that budget_2013 is the most important feature in the model which stands to reason as we have noticed this variable in aforementioned considerations. Movie ratings (imdb highest, metascore much lower but significant) are also important, as one can imagine given audience will provide feedback on a movie in a way that the criteria may well be relevant; while the domestic and international gross profit of movies are both similarly and significantly important as one would expect given that the data contain hollywood movies that are targetted similarly for an international and domestic audience (so both domestic as well as international gross profit should highly align) and the profits are high for where these criteria are met. The year of the movie is also an important consideration as one might imagine given that the year of a movie will reflect the culture and wishes of the target audience. Some other variables in the dataset (a few 'other' genre categories, rated_PG13) are also suggested by the model as being significant, albeit distinctly less significant than the other variables that have already been mentioned in this model / data interpretation.
#
# The overall confusion matrix shown above, for the results of applying the test data to the trained model, discriminates best for when the criteria Fail, which makes sense since more Fail than Pass. The corresponding ROC curve is certainly not perfect but it is considerably better than those that were shown for the evaluation results from some of the training models that were explored, and the gradient boosting model is much better than the case of random guessing (or a guess based on a naive model based on the simple proportioning of the data).  
```

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (Rmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: No-one, except that I sought advice from Kostis to understand what was required for the final exercise and then we went through that together in class (using 2-fold cross validation as a quick test). I subsequently re-ran this exercise with the full 10-fold cross validation and added my interpretation of the results for this last exercise.
-   Approximately how much time did you spend on this problem set: The full time of workshop 4 as well as 6 more hours of personal time in working on the problem and understanding the underpinning methods and analysis and interpretation.
-   What, if anything, gave you the most trouble: The biggest struggle was the find the questions, as there was a lot of pre-prepared code in which the questions were embedded. I hope that I found all of the questions. I also needed to check my english comprehension of a few of the questions as a few seemed to be ambiguous to me; once I understood what was required in terms of the question, I was fine. I also needed to check wherefrom all the extra code came, e.g. in the last exercise. Once I had worked out that, everything made sense and all was fine from a coding and data interpretation perspective.

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
